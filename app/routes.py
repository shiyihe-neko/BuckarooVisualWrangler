# Buckaroo Project - Final Stable Version
import numpy as np
import pandas as pd
from flask import request, render_template, send_from_directory, Response
import os
import time
from app import app
from app import connection, engine
from app.service_helpers import clean_table_name, get_whole_table_query, run_detectors, create_error_dict, \
    init_session_data_state, fetch_detected_and_undetected_current_dataset_from_db, calculate_attribute_rankings
from app import data_state_manager
from app.set_id_column import set_id_column
import json
from sqlalchemy import inspect, text

# --- æ ¸å¿ƒä¼˜åŒ–ï¼šå¤§å—æ…¢é€Ÿå†™å…¥ (å‡å°‘ SSL æ¡æ‰‹æ¬¡æ•°) ---
def safe_write_to_db_with_sleep(df, table_name, engine, chunk_size=2000):
    """
    ç­–ç•¥è°ƒæ•´ï¼š
    å¢å¤§ Chunk (2000) = å‡å°‘ç½‘ç»œè¯·æ±‚æ¬¡æ•° = å‡å°‘ SSL æŠ¥é”™æ¦‚ç‡
    å¢åŠ  Sleep (1.0s) = ç»™æ•°æ®åº“å……è¶³çš„ I/O ç¼“å†²æ—¶é—´
    """
    total_rows = len(df)
    print(f"ğŸš€ Starting SAFE WRITE for {table_name}: {total_rows} rows...")
    
    # è·å–è¿­ä»£å™¨ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
    # ç¬¬ä¸€æ¬¡å†™å…¥ (Replace)
    try:
        first_chunk = df.iloc[0:chunk_size]
        use_index = "rankings" not in table_name
        first_chunk.to_sql(table_name, engine, if_exists='replace', index=use_index)
        print(f"   Written first {chunk_size} rows...")
        time.sleep(1) # ä¼‘æ¯ 1 ç§’
        
        # åç»­å†™å…¥ (Append)
        for i in range(chunk_size, total_rows, chunk_size):
            chunk = df.iloc[i : i + chunk_size]
            chunk.to_sql(table_name, engine, if_exists='append', index=use_index)
            print(f"   Written chunk starting at {i}...")
            time.sleep(1) # æ¯æ¬¡å†™å®Œä¼‘æ¯ 1 ç§’
            
    except Exception as e:
        print(f"âŒ Write failed for {table_name}: {e}")
        raise e # æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿å¤–å±‚æ•è·

    print(f"ğŸ‰ Finished writing {table_name}!")

# --- è‡ªåŠ¨åŠ è½½ä¸ä¿®å¤é€»è¾‘ ---
def initialize_dataset_if_needed(cleaned_table_name, original_filename):
    inspector = inspect(engine)
    has_main = inspector.has_table(cleaned_table_name)
    has_error = inspector.has_table("errors" + cleaned_table_name)

    # åªè¦æœ‰ä»»ä½•ä¸€å¼ è¡¨ç¼ºå¤±ï¼Œæˆ–è€…å¤„äºä¸ä¸€è‡´çŠ¶æ€ï¼Œå°±é‡æ–°åŠ è½½
    if not has_main or not has_error:
        print(f"âš ï¸ Data mismatch for {cleaned_table_name}. Starting clean reload...")
        
        # 1. å…ˆå¼ºåˆ¶æ¸…ç†ç¯å¢ƒ (è§£å†³ relation does not exist é—®é¢˜)
        try:
            with engine.connect() as conn:
                trans = conn.begin()
                conn.execute(text(f'DROP TABLE IF EXISTS "{cleaned_table_name}" CASCADE;'))
                conn.execute(text(f'DROP TABLE IF EXISTS "errors{cleaned_table_name}" CASCADE;'))
                conn.execute(text(f'DROP TABLE IF EXISTS "rankings{cleaned_table_name}" CASCADE;'))
                trans.commit()
            print("   Cleaned up old tables.")
        except Exception as e:
            print(f"   Cleanup warning: {e}")

        # 2. å¯»æ‰¾æ–‡ä»¶
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
        paths = [
            os.path.join(base_dir, 'provided_datasets', original_filename),
            os.path.join(base_dir, 'app', 'static', 'provided_datasets', original_filename)
        ]
        csv_path = next((p for p in paths if os.path.exists(p)), None)
             
        if csv_path:
            try:
                print(f"ğŸ“– Reading CSV: {original_filename}")
                df = pd.read_csv(csv_path)
                
                print("ğŸ” Running detectors...")
                df_with_id = set_id_column(df)
                detected_data = run_detectors(df)
                
                # 3. æ‰§è¡Œå®‰å…¨å†™å…¥
                safe_write_to_db_with_sleep(df_with_id, cleaned_table_name, engine)
                safe_write_to_db_with_sleep(detected_data, "errors" + cleaned_table_name, engine)
                
                rankings = calculate_attribute_rankings(detected_data)
                rankings.to_sql("rankings" + cleaned_table_name, engine, if_exists='replace', index=False)
                
                print(f"âœ… Successfully loaded {cleaned_table_name}")
            except Exception as e:
                print(f"âŒ Failed to auto-load dataset: {e}")
                # å†æ¬¡æ¸…ç†ï¼Œé˜²æ­¢ç•™ä¸‹åŠæˆå“
                with engine.connect() as conn:
                    conn.execute(text(f'DROP TABLE IF EXISTS "{cleaned_table_name}" CASCADE;'))
        else:
            print(f"âŒ CSV file not found: {original_filename}")

# --- è·¯ç”±å®šä¹‰ ---

@app.post("/api/upload")
def upload_csv():
    try:
        csv_file = request.files['file']
        dataframe = pd.read_csv(csv_file)
        table_with_id_added = set_id_column(dataframe)
        
        start_time = time.time()
        detected_data = run_detectors(dataframe)
        time_to_detect = time.time() - start_time
        
        cleaned_table_name = clean_table_name(csv_file.filename)
        if not os.path.exists("report"): os.makedirs("report")
        json.dump({'db': cleaned_table_name, "clean_time": time_to_detect, "dataframe_shape": list(detected_data.shape)}, open(f"report/{cleaned_table_name}.json", "w"))

        safe_write_to_db_with_sleep(table_with_id_added, cleaned_table_name, engine)
        safe_write_to_db_with_sleep(detected_data, "errors"+cleaned_table_name, engine)
        
        rankings = calculate_attribute_rankings(detected_data)
        rankings.to_sql("rankings"+cleaned_table_name, engine, if_exists='replace', index=False)

        return{"success": True, "rows for undetected data": len(table_with_id_added), "clean_table_name": cleaned_table_name}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/get-sample")
def get_sample():
    filename = request.args.get("filename")
    data_size = request.args.get("datasize")
    cleaned_table_name = clean_table_name(filename)
    if not filename: return {"success": False, "error": "Filename required"}
    
    try:
        initialize_dataset_if_needed(cleaned_table_name, filename)
    except Exception as e:
        print(f"Init Error: {e}")

    QUERY = get_whole_table_query(cleaned_table_name,False) + " LIMIT "+ data_size
    try:
        fetch_detected_and_undetected_current_dataset_from_db(cleaned_table_name,engine)
        return pd.read_sql_query(QUERY, engine).replace(np.nan, None).to_dict(orient="records")
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/api/get-errors")
def get_errors():
    filename = request.args.get("filename")
    data_size = request.args.get("datasize")
    cleaned_table_name = clean_table_name(filename)
    if not filename: return {"success": False, "error": "Filename required"}
    
    try:
        initialize_dataset_if_needed(cleaned_table_name, filename)
    except: pass

    query = get_whole_table_query(cleaned_table_name,True)
    try:
        full_error_df = pd.read_sql_query(query, engine)
        return create_error_dict(full_error_df, int(data_size))
    except Exception as e:
        return {"success": False, "error": str(e)}

# --- ADMIN ROUTES ---

@app.route('/admin')
def admin_panel():
    return render_template('admin.html')

@app.route('/api/admin/reset_dataset')
def reset_dataset():
    filename = request.args.get('filename')
    if not filename: return "Filename required", 400
    cleaned_name = clean_table_name(filename)
    
    try:
        with engine.connect() as conn:
            trans = conn.begin()
            for table in [cleaned_name, "errors"+cleaned_name, "rankings"+cleaned_name]:
                conn.execute(text(f'DROP TABLE IF EXISTS "{table}" CASCADE;'))
            trans.commit()
        return {"success": True, "message": f"Dataset {cleaned_name} reset."}
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.route('/api/admin/download_table')
def download_table():
    table_name = request.args.get('table')
    if not table_name: return "Table name required", 400
    clean_name = clean_table_name(table_name)
    try:
        df = pd.read_sql_table(clean_name, engine)
        csv = df.to_csv(index=False)
        return Response(csv, mimetype="text/csv", headers={"Content-disposition": f"attachment; filename={clean_name}_cleaned.csv"})
    except Exception as e:
        return str(e), 500

# --- Standard Routes ---
@app.get("/")
def home(): return render_template('index.html')

@app.get('/data_cleaning_vis_tool')
def data_cleaning_vis_tool(): return render_template('data_cleaning_vis_tool.html')

@app.route('/tool')
def tool(): return render_template('data_cleaning_vis_tool.html')

# --- Static Files ---
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
@app.route('/detectors/<path:filename>')
def serve_detectors(filename): return send_from_directory(os.path.join(BASE_DIR, 'detectors'), filename)
@app.route('/wranglers/<path:filename>')
def serve_wranglers(filename): return send_from_directory(os.path.join(BASE_DIR, 'wranglers'), filename)
@app.route('/provided_datasets/<path:filename>')
def serve_datasets(filename): return send_from_directory(os.path.join(BASE_DIR, 'provided_datasets'), filename)
@app.route('/<filename>.json')
def serve_root_json(filename): return send_from_directory(BASE_DIR, f"{filename}.json")